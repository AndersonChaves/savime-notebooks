{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predição de modelos por meio de SAVIME e PYSAVIME \n",
    "\n",
    "__O objetivo deste notebook é demonstrar o recurso de predição de execução de modelos, \n",
    "por meio do sistema SAVIME em conjunto com a biblioteca de acesso PySavime. Serão utilizados os modelos criados no notebook Tutorial - Parte 01.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para execução deste tutorial é necessário que esteja inicializado o servidor tensorflow na porta 8501, bem como em execução o sistema Savime, na porta 65000. As dependências necessárias são especificadas nos notebooks anteriores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Definição dos diretórios e informações de configuração:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Necessário mudar o diretório de trabalho para o diretório raiz\n",
    "if not 'notebooks' in os.listdir('.'):\n",
    "    current_dir = os.path.abspath(os.getcwd())\n",
    "    parent_dir = os.path.dirname(current_dir)\n",
    "    os.chdir(parent_dir)\n",
    "\n",
    "# Informado o caminho da biblioteca do savime\n",
    "py_savime_path =  '/home/anderson/Programacao/Savime/pysavime'\n",
    "sys.path.insert(0, py_savime_path)\n",
    "\n",
    "# Informado o caminho do arquivo de dados: um json contendo informações a respeito \n",
    "# da partição de x e y utilizada na parte 01.\n",
    "data_file = 'saved_models_elastic_net/data.json'\n",
    "\n",
    "# Configuração do host e porta em que o SAVIME está escutando\n",
    "host = '127.0.0.1'\n",
    "port = 65000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Inclusão dos pacotes necessários para a execução do notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Savime imports\n",
    "from savime.client import Client\n",
    "import schema.define as savime_define\n",
    "import misc.commands as savime_op\n",
    "from util.converter import DataVariableBlockConverter\n",
    "from util.data_variable import DataVariableBlockOps\n",
    "\n",
    "from src.predictor_consumer import PredictionConsumer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Carga dos dados a serem utilizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array X\n",
      "[[-0.99998923 -0.9999818 ]\n",
      " [-0.99997866 -0.99996557]\n",
      " [-0.99996234 -0.99994934]\n",
      " ...\n",
      " [ 0.99995965  0.99997022]\n",
      " [ 0.9999744   0.99997724]\n",
      " [ 0.99998446  0.99998447]]\n",
      "Array Y\n",
      "[-1.98935628 -1.8733491  -2.05083874 ...  6.03294597  5.93841993\n",
      "  6.00459001]\n"
     ]
    }
   ],
   "source": [
    "# Leitura do arquivo de dados .json de entrada\n",
    "with open(data_file, 'r') as _in:\n",
    "    data = json.load(_in)\n",
    "\n",
    "# Leitura dos arrays X e Y\n",
    "output_dir = data['output_dir']\n",
    "x_fp = os.path.join(output_dir, data['x_file_name'])\n",
    "y_fp = os.path.join(output_dir, data['y_file_name'])\n",
    "\n",
    "x_array = np.load(x_fp)\n",
    "y_array = np.load(y_fp)\n",
    "\n",
    "print(\"Array X\")\n",
    "print(x_array)\n",
    "\n",
    "print(\"Array Y\")\n",
    "print(y_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_c_fp = os.path.join(output_dir, 'x_data')\n",
    "y_c_fp = os.path.join(output_dir, 'y_data')\n",
    "\n",
    "# Salvar os arrays numpy em arquivo sem os metadados do numpy.\n",
    "x_array.astype('float64').tofile(x_c_fp)\n",
    "y_array.astype('float64').tofile(y_c_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Preparação dos comandos a serem executados no Savime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE_DATASET(\"x:double:2\", \"@/home/anderson/Programacao/Savime/notebooks/savime-notebooks/saved_models_elastic_net/x_data\");\n",
      "CREATE_DATASET(\"y:double:1\", \"@/home/anderson/Programacao/Savime/notebooks/savime-notebooks/saved_models_elastic_net/y_data\");\n"
     ]
    }
   ],
   "source": [
    "# Definição dos datasets a serem utilizados:\n",
    "num_observations = 100000\n",
    "num_features     = 2\n",
    "y_num_columns    = 1 \n",
    "\n",
    "x_dataset = savime_define.file_dataset('x', x_c_fp, 'double', length=num_features)\n",
    "y_dataset = savime_define.file_dataset('y', y_c_fp, 'double')\n",
    "\n",
    "# O comando gerado será:\n",
    "print(x_dataset.create_query_str(), y_dataset.create_query_str(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE_TAR(\"tutorial\", \"*\", \"implicit, index, int32, 1, 100000, 1\", \"x, double: 2 | y, double: 1\");\n"
     ]
    }
   ],
   "source": [
    "# Definição do Tar a ser empregado:\n",
    "\n",
    "# Dimensão index\n",
    "index = savime_define.implicit_tar_dimension('index', 'int32', 1, num_observations)\n",
    "x = savime_define.tar_attribute('x', 'double', num_features)\n",
    "y = savime_define.tar_attribute('y', 'double', y_num_columns)\n",
    "tar = savime_define.tar('tutorial', [index], [x, y])\n",
    "\n",
    "# O comando gerado será:\n",
    "print(tar.create_query_str())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOAD_SUBTAR(\"tutorial\", \"ordered, index, #1,#100000\", \"x, x | y, y\")\n"
     ]
    }
   ],
   "source": [
    "# Carregamento do SubTar:\n",
    "# Aqui de fato os dados serão carregados no Tar.  \n",
    "\n",
    "subtar_index = savime_define.ordered_subtar_dimension(index, 1, num_observations)\n",
    "subtar_x = savime_define.subtar_attribute(x, x_dataset)\n",
    "subtar_y = savime_define.subtar_attribute(y, y_dataset)\n",
    "subtar = savime_define.subtar(tar, [subtar_index], [subtar_x, subtar_y])\n",
    "\n",
    "print(subtar.load_query_str())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execução dos comandos no Savime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-14 17:36:06 [Client]:ERROR: Query handler response message: Error during operation execution: Dataset x already exists.\n",
      "2020-02-14 17:36:06 [Client]:ERROR: Query handler response message: Error during operation execution: Dataset y already exists.\n",
      "2020-02-14 17:36:07 [Client]:ERROR: Query handler response message: Error during operation execution: tutorial TAR already exists.\n",
      "2020-02-14 17:36:07 [Client]:ERROR: Query handler response message: Error during operation execution: This new subtar definition intersects with already existing subtar!\n"
     ]
    }
   ],
   "source": [
    "# 1. Conexão é aberta e fechada com o SAVIME (contexto with)\n",
    "# 2. Criação de um objeto de execução de comandos vinculado à conexão criada.\n",
    "# 3. a) Criação dos datasets\n",
    "#    b) Criação do subtar\n",
    "#    c) Carregamento dos datasets por meio de um subtar\n",
    "\n",
    "with Client(host=host, port=port) as client:\n",
    "    client.execute(savime_op.create(x_dataset))\n",
    "    client.execute(savime_op.create(y_dataset))\n",
    "    client.execute(savime_op.create(tar))\n",
    "    client.execute(savime_op.load(subtar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Verificando se os comandos foram executados corretamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Os objetos abaixo são utilizados para converter do container mais genérico DataVariableBlock, retornado \n",
    "# como resposta da consulta, em  elmentos xarray e pandas. \n",
    "# Note que DataVariableBlocks contêm dois atributos: dims e attrs. Cada um desses contém um\n",
    "# dicionário do tipo nome_array: array, onde array é um numpy array.\n",
    "\n",
    "xarray_converter = DataVariableBlockConverter('xarray')\n",
    "pandas_converter = DataVariableBlockConverter('pandas')\n",
    "\n",
    "# Efetuar select no SAVIME a fim de verificar se o TAR for criado de forma adequada\n",
    "with Client(host=host, port=port) as client:\n",
    "    responses = client.execute(savime_op.select(tar))\n",
    "        \n",
    "# Em geral, o retorno do SAVIME a uma consulta é dado por subtar. Ou seja, se o tar contem n subtars então\n",
    "# a variável responses acima será uma lista com n DataVariableBlocks. Abaixo, eles são concatenados.\n",
    "data_variable_block = DataVariableBlockOps.concatenate(responses)\n",
    "\n",
    "# E abaixo convertidos\n",
    "xdataset_response = xarray_converter(data_variable_block)\n",
    "pandas_response = pandas_converter(data_variable_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XDATASET\n",
      "<xarray.Dataset>\n",
      "Dimensions:  (_0_: 2, index: 100000)\n",
      "Coordinates:\n",
      "  * index    (index) int32 1 2 3 4 5 6 ... 99995 99996 99997 99998 99999 100000\n",
      "  * _0_      (_0_) int64 0 1\n",
      "    _mask_   (index, _0_) bool True True True True True ... True True True True\n",
      "Data variables:\n",
      "    x        (index, _0_) float64 -1.0 -1.0 -1.0 -1.0 -1.0 ... 1.0 1.0 1.0 1.0\n",
      "    y        (index) float64 -1.989 -1.873 -2.051 -1.94 ... 6.033 5.938 6.005\n",
      "DATAFRAME\n",
      "               x                   y\n",
      "               0         1         0\n",
      "index                               \n",
      "1      -0.999989 -0.999982 -1.989356\n",
      "2      -0.999979 -0.999966 -1.873349\n",
      "3      -0.999962 -0.999949 -2.050839\n",
      "4      -0.999921 -0.999907 -1.939803\n",
      "5      -0.999897 -0.999871 -2.060704\n",
      "...          ...       ...       ...\n",
      "99996   0.999898  0.999903  5.969954\n",
      "99997   0.999909  0.999938  5.953251\n",
      "99998   0.999960  0.999970  6.032946\n",
      "99999   0.999974  0.999977  5.938420\n",
      "100000  0.999984  0.999984  6.004590\n",
      "\n",
      "[100000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Obs.: \n",
    "# 1) Como xarray representa arrays como matrizes multidimensionais densas e SAVIME é mais genérico, \n",
    "# abarcando matrizes esparsas, é preciso manter uma máscara _mask_ identificando se determinado\n",
    "# elemento na matriz está presente ou não.\n",
    "# 2) Dataframes pandas é intrinsicamente uma estrutura tabular. Para permitir a representação de atributos\n",
    "# com uma segunda dimensão (matrizes) emprega-se índices múltiplos. Note a coluna x abaixo.\n",
    "print('XDATASET')\n",
    "print(xdataset_response)\n",
    "print('DATAFRAME')\n",
    "print(pandas_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Instanciação TFX e execução de consultas preditivas\n",
    "\n",
    "### Para executar os passos abaixo é nessário que o servidor de modelos esteja rodando.\n",
    "\n",
    "Execute o comando abaixo:\n",
    "`tensorflow_model_server --rest_api_port=8501 --model_config_file=ARQUIVO_DE_MODELOS` \n",
    "Note que você deve trocar ARQUIVO_DE_MODELOS pelo caminho do arquivo no qual os modelos foram registrados. Esse arquivo é o `models.config` dentro da pasta `saved_models`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
